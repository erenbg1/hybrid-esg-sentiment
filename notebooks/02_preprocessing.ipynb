{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c2908ec-2309-489f-af4a-5bf366ce621d",
   "metadata": {},
   "source": [
    "### Text Preprocessing: Normalization and Sentence Splitting ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6fc742a-566d-4549-a59f-32b69f7bb857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed corpus saved to: data/cleaned_v2/preprocessed_corpus.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "\n",
    "# Load English model for sentence splitting + lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Paths\n",
    "corpus_path = Path(\"data/cleaned_v2/combined_corpus.jsonl\")\n",
    "output_path = Path(\"data/cleaned_v2/preprocessed_corpus.jsonl\")\n",
    "\n",
    "def normalize_numbers_units(text: str) -> str:\n",
    "    # Remove thousands separators in numbers: 50,000 -> 50000\n",
    "    text = re.sub(r\"(\\d),(\\d{3})\", r\"\\1\\2\", text)\n",
    "    # Normalize percentages: 20 % -> 20%\n",
    "    text = re.sub(r\"(\\d+)\\s?%\", r\"\\1%\", text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text: str) -> list:\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Normalize numbers and units\n",
    "    text = normalize_numbers_units(text)\n",
    "    # Collapse multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # Sentence splitting with spaCy\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    return sentences\n",
    "\n",
    "with open(corpus_path, \"r\", encoding=\"utf-8\") as f_in, \\\n",
    "     open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    \n",
    "    for line in f_in:\n",
    "        rec = json.loads(line)\n",
    "        rec[\"sentences\"] = preprocess_text(rec[\"text\"])\n",
    "        # Drop original full text if you want to save space\n",
    "        # del rec[\"text\"]\n",
    "        f_out.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Preprocessed corpus saved to:\", output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7f587d-e5c3-436a-81aa-c7c825d2d8af",
   "metadata": {},
   "source": [
    "### Text Preprocessing: Stopword Removal and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d10b05d-7a76-43b5-90fd-ed86ad7a855d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reduce', 'emission', 'renewable', 'initiative']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])  # NER kapalı, hız için\n",
    "\n",
    "def preprocess_with_lemma_and_stopwords(text: str):\n",
    "    \"\"\"\n",
    "    Apply lowercasing, number normalization, lemmatization,\n",
    "    and stopword removal using spaCy.\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Normalize numbers\n",
    "    text = re.sub(r\"(\\d),(\\d{3})\", r\"\\1\\2\", text)   # 50,000 → 50000\n",
    "    text = re.sub(r\"(\\d+)\\s?%\", r\"\\1%\", text)       # 20 % → 20%\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # Process with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Keep only non-stopwords, alphabetic tokens, and lemmatize\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc \n",
    "        if not token.is_stop and token.is_alpha\n",
    "    ]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"We are reducing our CO2 emissions by 20% through renewable initiatives.\"\n",
    "print(preprocess_with_lemma_and_stopwords(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548de15a-135d-4fdc-86c7-4c66799a31ad",
   "metadata": {},
   "source": [
    "### Creating Lemmatized Corpus for Lexicon/Snippet Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91d9dd71-7f5f-4103-a5b9-a3b831ac4a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon/Snippet corpus created at: data/cleaned_v2/preprocessed_with_lemma.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
    "\n",
    "# Paths\n",
    "input_file = Path(\"data/cleaned_v2/combined_corpus.jsonl\")\n",
    "output_file_tokens = Path(\"data/cleaned_v2/preprocessed_with_lemma.jsonl\")\n",
    "\n",
    "def normalize_numbers(text: str) -> str:\n",
    "    text = re.sub(r\"(\\d),(\\d{3})\", r\"\\1\\2\", text)   # 50,000 -> 50000\n",
    "    text = re.sub(r\"(\\d+)\\s?%\", r\"\\1%\", text)       # 20 % -> 20%\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def preprocess_tokens(text: str):\n",
    "    \"\"\"Lowercasing + number normalization + lemmatization + stopword removal\"\"\"\n",
    "    text = text.lower()\n",
    "    text = normalize_numbers(text)\n",
    "    doc = nlp(text)\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc\n",
    "        if not token.is_stop and token.is_alpha\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "# Create corpus\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f_in, \\\n",
    "     open(output_file_tokens, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    \n",
    "    for line in f_in:\n",
    "        rec = json.loads(line)\n",
    "        rec_tok = {\n",
    "            \"company\": rec[\"company\"],\n",
    "            \"year\": rec[\"year\"],\n",
    "            \"file\": rec[\"file\"],\n",
    "            \"tokens\": preprocess_tokens(rec[\"text\"])\n",
    "        }\n",
    "        f_out.write(json.dumps(rec_tok, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Lexicon/Snippet corpus created at:\", output_file_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2db8ef-10f9-465f-a6f1-ea6d54b978df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (spacy)",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
